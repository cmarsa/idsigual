{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __10 Statistical Natural Language Processing for Sentiment Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, sentiment analysis is performed based on the processing of natural\n",
    "language, the analysis of text and computational linguistics. Although data can come\n",
    "from different data sources, in this chapter we will analyze sentiment in text data,\n",
    "using two particular text data examples: one from film critics, where the text is highly\n",
    "structured and maintains text semantics; and another example coming from social\n",
    "networks (tweets in this case), where the text can show a lack of structure and users\n",
    "may use (and abuse!) text abbreviations.\n",
    "\n",
    "In the following sections, we will review some basic mechanisms required to\n",
    "perform sentiment analysis. In particular, we will analyze the steps required for\n",
    "data cleaning (that is, removing irrelevant text items not associated with sentiment\n",
    "information), producing a general representation of the text, and performing some\n",
    "statistical inference on the text represented to determine positive and negative senti-\n",
    "ments.\n",
    "\n",
    "Although the scope of sentiment analysis may introduce many aspects to be ana-\n",
    "lyzed, in this chapter and for simplicity, we will analyze binary sentiment analysis\n",
    "categorization problems. We will thus basically learn to classify positive against\n",
    "negative opinions from text data. The scope of sentiment analysis is broader, and it\n",
    "includes many aspects that make analysis of sentiments a challenging task. Some\n",
    "interesting open issues in this topic are as follows:\n",
    "\n",
    "+ Identification of sarcasm: sometimes without knowing the personality of the per-\n",
    "    son, you do not know whether “bad” means bad or good.\n",
    "+ Lack of text structure: in the case of Twitter, for example, it may contain abbre-\n",
    "    viations, and there may be a lack of capitals, poor spelling, poor punctuation, and\n",
    "    poor grammar, all of which make it difficult to analyze the text.\n",
    "+ Many possible sentiment categories and degrees: positive and negative is a simple\n",
    "    analysis, one would like to identify the amount of hate there is inside the opinion,\n",
    "    how much happiness, how much sadness, etc.\n",
    "+ Identification of the object of analysis: many concepts can appear in text, and how\n",
    "    to detect the object that the opinion is positive for and the object that the opinion is\n",
    "    negative for is an open issue. For example, if you say “She won him!”, this means\n",
    "    a positive sentiment for her and a negative sentiment for him, at the same time.\n",
    "+ Subjective text: another open challenge is how to analyze very subjective sentences\n",
    "    or paragraphs. Sometimes, even for humans it is very hard to agree on the sentiment\n",
    "    of these highly subjective texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data Cleaning__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main task of data cleaning is to remove\n",
    "those characters considered as noise in the data mining process. For instance, comma\n",
    "or colon characters. Of course, in each particular data mining problem different char-\n",
    "acters can be considered as noise, depending on the final objective of the analysis. In\n",
    "our case, we are going to consider that all punctuation characters should be removed,\n",
    "including other non-conventional symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\n",
    "    'Here are some very simple basic sentences.',\n",
    "    'They won`t be very interesting, I`m afraid.',\n",
    "    'The point of these examples i to _learn how basoc text\\ cleaning works_ on *very simple* data. '\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step consists of defining a list with all word-vectors in the text.\n",
    "`NLTK` makes it easy to convert documents-as-trings into word-vectors, a process\n",
    "called tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'won', '`', 't', 'be', 'very', 'interesting', ',', 'I', '`', 'm', 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'i', 'to', '_learn', 'how', 'basoc', 'text\\\\', 'cleaning', 'works_', 'on', '*', 'very', 'simple', '*', 'data', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for each line of text in raw_docs, word_tokenize function will set\n",
    "the list of word-vectors. Now we can search the list for punctuation symbols, for\n",
    "instance, and remove them. There are many ways to perform this step. Let us see\n",
    "one possible solution using the `String` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'won', 't', 'be', 'very', 'interesting', 'I', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'i', 'to', 'learn', 'how', 'basoc', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "regex = re.compile(f'[{re.escape(string.punctuation)}]')\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important step in many data mining systems for text analysis consists of\n",
    "stemming and lemmatizing. Morphology is the notion that words have a root form.\n",
    "If you want to get to the basic term meaning of the word, you can try applying\n",
    "a stemmer or lemmatizer. This step is useful to reduce the dictionary size and the\n",
    "posterior high-dimensional and sparse feature spaces. NLTK provides different ways\n",
    "of performing this procedure. In the case of running the porter.stem(word)\n",
    "approach, the output is shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'won', 't', 'be', 'very', 'interesting', 'I', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'i', 'to', 'learn', 'how', 'basoc', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n",
      "[['here', 'are', 'some', 'veri', 'simpl', 'basic', 'sentenc'], ['they', 'won', 't', 'be', 'veri', 'interest', 'I', 'm', 'afraid'], ['the', 'point', 'of', 'these', 'exampl', 'i', 'to', 'learn', 'how', 'basoc', 'text', 'clean', 'work', 'on', 'veri', 'simpl', 'data']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        final_doc.append(porter.stem(word))\n",
    "    preprocessed_docs.append(final_doc)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)\n",
    "print(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of approaches are very useful in order to reduce the exponential number\n",
    "of combinations of words with the same meaning and match similar texts. Words\n",
    "such as “interest” and “interesting” will be converted into the same word “interest”\n",
    "making the comparison of texts easier, as we will see later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Text Representation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we have analyzed different techniques for data cleaning, stem-\n",
    "ming, and lemmatizing, and filtering the text to remove other unnecessary tags for\n",
    "posterior text analysis. In order to analyze sentiment from text, the next step consists\n",
    "of having a representation of the text that has been cleaned.\n",
    "Although different reprresentations of text exist, the most common ones are variants of Bag of Words (BoW) models.\n",
    "\n",
    "he basic idea is to think about word frequencies. If we can define a\n",
    "dictionary of possible different words, the number of different existing words will\n",
    "define the length of a feature space to represent each text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will see a particular case of bag of words, the Vector Space Model of\n",
    "text: TF–IDF (term frequency–inverse distance frequency). First, we need to count\n",
    "the terms per document, which is the term frequency vector. See a code example\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('Mireia', 1), ('loves', 2), ('me', 2), ('more', 1), ('than', 1), ('Hector', 1)])\n",
      "dict_items([('Sergio', 1), ('likes', 1), ('me', 2), ('more', 1), ('than', 1), ('Mireia', 1), ('loves', 1)])\n",
      "dict_items([('HE', 1), ('likes', 1), ('basketball', 1), ('more', 1), ('than', 1), ('football', 1)])\n"
     ]
    }
   ],
   "source": [
    "mydoclist = [\n",
    "    'Mireia loves me more than Hector loves me',\n",
    "    'Sergio likes me more than Mireia loves me',\n",
    "    'HE likes basketball more than football'\n",
    "]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "for doc in mydoclist:\n",
    "    tf = Counter()\n",
    "    for word in doc.split():\n",
    "        tf[word] += 1\n",
    "    print(tf.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us call this a first stab at representing documents quantitatively, just by their\n",
    "word counts (also thinking that we may have previously filtered and cleaned the text\n",
    "using previous approaches). Here we show an example for computing the feature\n",
    "vector based on word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lexicon(corpus):\n",
    "    '''Define a set with all possible words included in all the\n",
    "    sentences or corpus.\n",
    "    '''\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    return lexicon\n",
    "\n",
    "def freq(term, document):\n",
    "    return document.split().count(term)\n",
    "\n",
    "def tf(term, document):\n",
    "    return freq(term, document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out vocabulary vector is [Hector, Sergio, Mireia, more, likes, football, me, HE, loves, than, basketball]\n",
      "the doc is \"Mireia loves me more than Hector loves me\"\n",
      "the tf vector for Document 1 is [1, 0, 1, 1, 0, 0, 2, 0, 2, 1, 0]\n",
      "the doc is \"Sergio likes me more than Mireia loves me\"\n",
      "the tf vector for Document 2 is [0, 1, 1, 1, 1, 0, 2, 0, 1, 1, 0]\n",
      "the doc is \"HE likes basketball more than football\"\n",
      "the tf vector for Document 3 is [0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1]\n",
      "all combined, here is our master document term matrix: \n",
      "[[1, 0, 1, 1, 0, 0, 2, 0, 2, 1, 0], [0, 1, 1, 1, 1, 0, 2, 0, 1, 1, 0], [0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = build_lexicon(mydoclist)\n",
    "doc_term_matrix = []\n",
    "\n",
    "print('out vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "\n",
    "for doc in mydoclist:\n",
    "    print('the doc is \"' + doc + '\"')\n",
    "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
    "    print(f'the tf vector for Document {mydoclist.index(doc) + 1} is [{tf_vector_string}]')\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "\n",
    "print('all combined, here is our master document term matrix: ')\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, every document is in the same feature space, meaning that we can represent\n",
    "the entire corpus in the same dimensional space. Once we have the data in the\n",
    "same feature space, we can start applying some machine learning methods: learning,\n",
    "classifying, clustering, and so on. But actually, we have a few problems. Words are\n",
    "not all equally informative. If words appear too frequently in a single document,\n",
    "they are going to muck up our analysis. We want to perform some weighting of these\n",
    "term frequency vectors into something a bit more representative. That is, we need to\n",
    "do some vector normalizing. One possibility is to ensure that the L2 norm of each\n",
    "vector is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a regular old document term matrix: \n",
      "[[1 0 1 1 0 0 2 0 2 1 0]\n",
      " [0 1 1 1 1 0 2 0 1 1 0]\n",
      " [0 0 0 1 1 1 0 1 0 1 1]]\n",
      "\n",
      "A document term matrix with row-wise L2 norm: \n",
      "[[0.28867513 0.         0.28867513 0.28867513 0.         0.\n",
      "  0.57735027 0.         0.57735027 0.28867513 0.        ]\n",
      " [0.         0.31622777 0.31622777 0.31622777 0.31622777 0.\n",
      "  0.63245553 0.         0.31622777 0.31622777 0.        ]\n",
      " [0.         0.         0.         0.40824829 0.40824829 0.40824829\n",
      "  0.         0.40824829 0.         0.40824829 0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def l2_normalizer(vec):\n",
    "    denom = np.sum([el ** 2 for el in vec])\n",
    "    return [(el / math.sqrt(denom)) for el in vec]\n",
    "\n",
    "doc_term_matrix_l2 = []\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "    \n",
    "print('a regular old document term matrix: ')\n",
    "print(np.matrix(doc_term_matrix))\n",
    "\n",
    "print('\\nA document term matrix with row-wise L2 norm: ')\n",
    "print(np.matrix(doc_term_matrix_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have scaled down the vectors so that each element is between\n",
    "[0, 1]. This will avoid getting a diminishing return on the informative value of a word\n",
    "massively used in a particular document. For that, we need to scale down words that\n",
    "appear too frequently in a document.\n",
    "\n",
    "Finally, we have a final task to perform. Just as not all words are equally valuable\n",
    "within a document, not all words are valuable across all documents. We can try\n",
    "reweighting every word by its inverse document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('lab': virtualenv)",
   "language": "python",
   "name": "python37464bitlabvirtualenved77d976613b4753a84284c005b6ce98"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
