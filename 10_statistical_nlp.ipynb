{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __10 Statistical Natural Language Processing for Sentiment Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, sentiment analysis is performed based on the processing of natural\n",
    "language, the analysis of text and computational linguistics. Although data can come\n",
    "from different data sources, in this chapter we will analyze sentiment in text data,\n",
    "using two particular text data examples: one from film critics, where the text is highly\n",
    "structured and maintains text semantics; and another example coming from social\n",
    "networks (tweets in this case), where the text can show a lack of structure and users\n",
    "may use (and abuse!) text abbreviations.\n",
    "\n",
    "In the following sections, we will review some basic mechanisms required to\n",
    "perform sentiment analysis. In particular, we will analyze the steps required for\n",
    "data cleaning (that is, removing irrelevant text items not associated with sentiment\n",
    "information), producing a general representation of the text, and performing some\n",
    "statistical inference on the text represented to determine positive and negative senti-\n",
    "ments.\n",
    "\n",
    "Although the scope of sentiment analysis may introduce many aspects to be ana-\n",
    "lyzed, in this chapter and for simplicity, we will analyze binary sentiment analysis\n",
    "categorization problems. We will thus basically learn to classify positive against\n",
    "negative opinions from text data. The scope of sentiment analysis is broader, and it\n",
    "includes many aspects that make analysis of sentiments a challenging task. Some\n",
    "interesting open issues in this topic are as follows:\n",
    "\n",
    "+ Identification of sarcasm: sometimes without knowing the personality of the per-\n",
    "    son, you do not know whether “bad” means bad or good.\n",
    "+ Lack of text structure: in the case of Twitter, for example, it may contain abbre-\n",
    "    viations, and there may be a lack of capitals, poor spelling, poor punctuation, and\n",
    "    poor grammar, all of which make it difficult to analyze the text.\n",
    "+ Many possible sentiment categories and degrees: positive and negative is a simple\n",
    "    analysis, one would like to identify the amount of hate there is inside the opinion,\n",
    "    how much happiness, how much sadness, etc.\n",
    "+ Identification of the object of analysis: many concepts can appear in text, and how\n",
    "    to detect the object that the opinion is positive for and the object that the opinion is\n",
    "    negative for is an open issue. For example, if you say “She won him!”, this means\n",
    "    a positive sentiment for her and a negative sentiment for him, at the same time.\n",
    "+ Subjective text: another open challenge is how to analyze very subjective sentences\n",
    "    or paragraphs. Sometimes, even for humans it is very hard to agree on the sentiment\n",
    "    of these highly subjective texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data Cleaning__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main task of data cleaning is to remove\n",
    "those characters considered as noise in the data mining process. For instance, comma\n",
    "or colon characters. Of course, in each particular data mining problem different char-\n",
    "acters can be considered as noise, depending on the final objective of the analysis. In\n",
    "our case, we are going to consider that all punctuation characters should be removed,\n",
    "including other non-conventional symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\n",
    "    'Here are some very simple basic sentences.',\n",
    "    'They won`t be very interesting, I`m afraid.',\n",
    "    'The point of these examples i to _learn how basoc text\\ cleaning works_ on *very simple* data. '\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step consists of defining a list with all word-vectors in the text.\n",
    "`NLTK` makes it easy to convert documents-as-trings into word-vectors, a process\n",
    "called tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'won', '`', 't', 'be', 'very', 'interesting', ',', 'I', '`', 'm', 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'i', 'to', '_learn', 'how', 'basoc', 'text\\\\', 'cleaning', 'works_', 'on', '*', 'very', 'simple', '*', 'data', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for each line of text in raw_docs, word_tokenize function will set\n",
    "the list of word-vectors. Now we can search the list for punctuation symbols, for\n",
    "instance, and remove them. There are many ways to perform this step. Let us see\n",
    "one possible solution using the `String` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'won', 't', 'be', 'very', 'interesting', 'I', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'i', 'to', 'learn', 'how', 'basoc', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "regex = re.compile(f'[{re.escape(string.punctuation)}]')\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important step in many data mining systems for text analysis consists of\n",
    "stemming and lemmatizing. Morphology is the notion that words have a root form.\n",
    "If you want to get to the basic term meaning of the word, you can try applying\n",
    "a stemmer or lemmatizer. This step is useful to reduce the dictionary size and the\n",
    "posterior high-dimensional and sparse feature spaces. NLTK provides different ways\n",
    "of performing this procedure. In the case of running the porter.stem(word)\n",
    "approach, the output is shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'won', 't', 'be', 'very', 'interesting', 'I', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'i', 'to', 'learn', 'how', 'basoc', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n",
      "[['here', 'are', 'some', 'veri', 'simpl', 'basic', 'sentenc'], ['they', 'won', 't', 'be', 'veri', 'interest', 'I', 'm', 'afraid'], ['the', 'point', 'of', 'these', 'exampl', 'i', 'to', 'learn', 'how', 'basoc', 'text', 'clean', 'work', 'on', 'veri', 'simpl', 'data']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        final_doc.append(porter.stem(word))\n",
    "    preprocessed_docs.append(final_doc)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)\n",
    "print(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of approaches are very useful in order to reduce the exponential number\n",
    "of combinations of words with the same meaning and match similar texts. Words\n",
    "such as “interest” and “interesting” will be converted into the same word “interest”\n",
    "making the comparison of texts easier, as we will see later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Text Representation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we have analyzed different techniques for data cleaning, stemming, and lemmatizing, and filtering the text to remove other unnecessary tags for\n",
    "posterior text analysis. In order to analyze sentiment from text, the next step consists\n",
    "of having a representation of the text that has been cleaned.\n",
    "Although different reprresentations of text exist, the most common ones are variants of Bag of Words (BoW) models.\n",
    "\n",
    "The basic idea is to think about word frequencies. If we can define a\n",
    "dictionary of possible different words, the number of different existing words will\n",
    "define the length of a feature space to represent each text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will see a particular case of bag of words, the Vector Space Model of\n",
    "text: TF–IDF (term frequency–inverse distance frequency). First, we need to count\n",
    "the terms per document, which is the term frequency vector. See a code example\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('Mireia', 1), ('loves', 2), ('me', 2), ('more', 1), ('than', 1), ('Hector', 1)])\n",
      "dict_items([('Sergio', 1), ('likes', 1), ('me', 2), ('more', 1), ('than', 1), ('Mireia', 1), ('loves', 1)])\n",
      "dict_items([('HE', 1), ('likes', 1), ('basketball', 1), ('more', 1), ('than', 1), ('football', 1)])\n"
     ]
    }
   ],
   "source": [
    "mydoclist = [\n",
    "    'Mireia loves me more than Hector loves me',\n",
    "    'Sergio likes me more than Mireia loves me',\n",
    "    'HE likes basketball more than football'\n",
    "]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "for doc in mydoclist:\n",
    "    tf = Counter()\n",
    "    for word in doc.split():\n",
    "        tf[word] += 1\n",
    "    print(tf.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us call this a first stab at representing documents quantitatively, just by their\n",
    "word counts (also thinking that we may have previously filtered and cleaned the text\n",
    "using previous approaches). Here we show an example for computing the feature\n",
    "vector based on word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lexicon(corpus):\n",
    "    '''Define a set with all possible words included in all the\n",
    "    sentences or corpus.\n",
    "    '''\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    return lexicon\n",
    "\n",
    "def freq(term, document):\n",
    "    return document.split().count(term)\n",
    "\n",
    "def tf(term, document):\n",
    "    return freq(term, document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out vocabulary vector is [Mireia, football, HE, me, loves, more, likes, than, basketball, Hector, Sergio]\n",
      "the doc is \"Mireia loves me more than Hector loves me\"\n",
      "the tf vector for Document 1 is [1, 0, 0, 2, 2, 1, 0, 1, 0, 1, 0]\n",
      "the doc is \"Sergio likes me more than Mireia loves me\"\n",
      "the tf vector for Document 2 is [1, 0, 0, 2, 1, 1, 1, 1, 0, 0, 1]\n",
      "the doc is \"HE likes basketball more than football\"\n",
      "the tf vector for Document 3 is [0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "all combined, here is our master document term matrix: \n",
      "[[1, 0, 0, 2, 2, 1, 0, 1, 0, 1, 0], [1, 0, 0, 2, 1, 1, 1, 1, 0, 0, 1], [0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = build_lexicon(mydoclist)\n",
    "doc_term_matrix = []\n",
    "\n",
    "print('out vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "\n",
    "for doc in mydoclist:\n",
    "    print('the doc is \"' + doc + '\"')\n",
    "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
    "    print(f'the tf vector for Document {mydoclist.index(doc) + 1} is [{tf_vector_string}]')\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "\n",
    "print('all combined, here is our master document term matrix: ')\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, every document is in the same feature space, meaning that we can represent\n",
    "the entire corpus in the same dimensional space. Once we have the data in the\n",
    "same feature space, we can start applying some machine learning methods: learning,\n",
    "classifying, clustering, and so on. But actually, we have a few problems. Words are\n",
    "not all equally informative. If words appear too frequently in a single document,\n",
    "they are going to muck up our analysis. We want to perform some weighting of these\n",
    "term frequency vectors into something a bit more representative. That is, we need to\n",
    "do some vector normalizing. One possibility is to ensure that the L2 norm of each\n",
    "vector is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a regular old document term matrix: \n",
      "[[1 0 0 2 2 1 0 1 0 1 0]\n",
      " [1 0 0 2 1 1 1 1 0 0 1]\n",
      " [0 1 1 0 0 1 1 1 1 0 0]]\n",
      "\n",
      "A document term matrix with row-wise L2 norm: \n",
      "[[0.28867513 0.         0.         0.57735027 0.57735027 0.28867513\n",
      "  0.         0.28867513 0.         0.28867513 0.        ]\n",
      " [0.31622777 0.         0.         0.63245553 0.31622777 0.31622777\n",
      "  0.31622777 0.31622777 0.         0.         0.31622777]\n",
      " [0.         0.40824829 0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.40824829 0.40824829 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def l2_normalizer(vec):\n",
    "    denom = np.sum([el ** 2 for el in vec])\n",
    "    return [(el / math.sqrt(denom)) for el in vec]\n",
    "\n",
    "doc_term_matrix_l2 = []\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "    \n",
    "print('a regular old document term matrix: ')\n",
    "print(np.matrix(doc_term_matrix))\n",
    "\n",
    "print('\\nA document term matrix with row-wise L2 norm: ')\n",
    "print(np.matrix(doc_term_matrix_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have scaled down the vectors so that each element is between\n",
    "[0, 1]. This will avoid getting a diminishing return on the informative value of a word\n",
    "massively used in a particular document. For that, we need to scale down words that\n",
    "appear too frequently in a document.\n",
    "\n",
    "Finally, we have a final task to perform. Just as not all words are equally valuable\n",
    "within a document, not all words are valuable across all documents. We can try\n",
    "reweighting every word by its inverse document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out vocabulary vector is [Mireia, football, HE, me, loves, more, likes, than, basketball, Hector, Sergio]\n",
      "The inverse document frequency vector is [0.405465, 1.098612, 1.098612, 0.405465, 0.405465, 0.000000, 0.405465, 0.000000, 1.098612, 1.098612, 1.098612]\n"
     ]
    }
   ],
   "source": [
    "def num_docs_containing(word, doclist):\n",
    "    doccount = 0\n",
    "    for doc in doclist:\n",
    "        if freq(word, doc) > 0:\n",
    "            doccount += 1\n",
    "    return doccount\n",
    "\n",
    "def idf(word, doclist):\n",
    "    n_samples = len(doclist)\n",
    "    df = num_docs_containing(word, doclist)\n",
    "    return np.log(n_samples / float(df))\n",
    "\n",
    "\n",
    "my_idf_vector = [idf(word, mydoclist) for word in vocabulary]\n",
    "\n",
    "print('Out vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "print('The inverse document frequency vector is [' + ', '.join(format(freq, 'f') for freq in my_idf_vector) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a general sense of information values per term in our vocabulary,\n",
    "accounting for their relative frequency across the entire corpus.\n",
    "Note that his is an inverse. To get TF-IDF weighted word-vectors, we have to perform\n",
    "the simple calculation of the term frequencies multiplied by the inverse frequency\n",
    "values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.40546511 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         1.09861229 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.09861229 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.40546511 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.40546511 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.40546511 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.09861229 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.09861229 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.09861229]]\n"
     ]
    }
   ],
   "source": [
    "def build_idf_matrix(idf_vector):\n",
    "    idf_mat = np.zeros((len(idf_vector), len(idf_vector)))\n",
    "    np.fill_diagonal(idf_mat, idf_vector)\n",
    "    return idf_mat\n",
    "\n",
    "my_idf_matrix = build_idf_matrix(my_idf_vector)\n",
    "\n",
    "print(my_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means we can now multiply every term frequency vector by the inverse\n",
    "document frequency matrix. Then, to make sure we are also accounting for words\n",
    "that appear too frequently within documents, we will normalize each document using\n",
    "the L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mireia', 'football', 'HE', 'me', 'loves', 'more', 'likes', 'than', 'basketball', 'Hector', 'Sergio'}\n",
      "[[0.24737436 0.         0.         0.49474872 0.49474872 0.\n",
      "  0.         0.         0.         0.67026363 0.        ]\n",
      " [0.2640605  0.         0.         0.52812101 0.2640605  0.\n",
      "  0.2640605  0.         0.         0.         0.71547492]\n",
      " [0.         0.56467328 0.56467328 0.         0.         0.\n",
      "  0.20840411 0.         0.56467328 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix_tfidf = []\n",
    "\n",
    "# performing tf-idf matrix multiplication\n",
    "for tf_vector in doc_term_matrix:\n",
    "    doc_term_matrix_tfidf.append(np.dot(tf_vector, my_idf_matrix))\n",
    "    \n",
    "# normalizing\n",
    "doc_term_matrix_tfidf_l2 = []\n",
    "for tf_vector in doc_term_matrix_tfidf:\n",
    "    doc_term_matrix_tfidf_l2.append(l2_normalizer(tf_vector))\n",
    "    \n",
    "print(vocabulary)\n",
    "\n",
    "print(np.matrix(doc_term_matrix_tfidf_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Bi-Grams and n-Grams__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sometimes useful to take significant bi-grams into the model based on the BoW.\n",
    "Note that this example can be extended to n-grams. In the fields of computational\n",
    "linguistics and probability, an n-gram is a contiguous sequence of n items from\n",
    "a given sequence of text or speech. The items can be phonemes, syllables, letters,\n",
    "words, or base pairs according to the application. The n-grams are typically collected\n",
    "from a text or speech corpus.\n",
    "\n",
    "A n-gram of size 1 is referred to as a “uni-gram”; size 2 is a “bi-gram” (or, less\n",
    "commonly, a “digram”); size 3 is a “tri-gram”. Larger sizes are sometimes referred\n",
    "to by the value of n, e.g., “four-gram”, “five-gram”, and so on. These n-grams can\n",
    "be introduced within the BoW model just by considering each different n-gram as a\n",
    "new position within the feature vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Practical Cases__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python packages provide useful tools for analyzing text. The reader is referred to\n",
    "the NLTK and Textblob package documentation for further details. Here, we will\n",
    "perform all the previously presented procedures for data cleaning, stemming, and\n",
    "representation and introduce some binary learning schemes to learn the text representations in the feature space. The binary learning schemes will receive examples\n",
    "for training positive and negative sentiment texts and we will apply them later to\n",
    "unseen examples from a test set.\n",
    "\n",
    "We will apply the whole sentiment analysis process in two examples. The first\n",
    "corresponds to the Large Movie reviews dataset. This is one of the largest public\n",
    "available data sets for sentiment analysis, which includes more than 50,000 texts\n",
    "from movie reviews including the groundtruth annotation related to positive and\n",
    "negative movie reviews. As a proof on concept, for this example we use a subset of\n",
    "the dataset consisting of about 30% of the data.\n",
    "\n",
    "The code reuses part of the previous examples for data cleaning, reads training\n",
    "and testing data from the folders as provided by the authors of the dataset. Then,\n",
    "TF–IDF is computed, which performs all steps mentioned previously for computing\n",
    "feature space, normalization, and feature weights. Note that at the end of the script we\n",
    "perform training and testing based on two different state-of-the-art machine learning\n",
    "approaches: Naive Bayes and Support Vector Machines. It is beyond the scope of\n",
    "this chapter to give details of the methods and parameters. The important point here\n",
    "is that the documents are represented in feature spaces that can be used by different\n",
    "data mining tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we apply the whole sentiment analysis process to the Large Movie reviews dataset (http://www.aclweb.org/anthology/P11-1015). This is one of the largest public available data sets for sentiment analysis, which includes more than 50.000 texts from movie reviews including the ground truth annotation related to positive and negative movie review. As a proof on concept for this example we use a subset of the dataset consisting in about 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "files=5\n",
    "count=0\n",
    "for file in os.listdir(\"files/ch10/aclImdb/train/pos/\"):\n",
    "    if count > files:\n",
    "        break\n",
    "    if file.endswith(\".txt\"):\n",
    "        os.rename('files/ch10/aclImdb/train/pos/' + file, 'files/ch10/train/pos2/' + file)\n",
    "    count=count+1\n",
    "count=0\n",
    "for file in os.listdir(\"files/ch10/aclImdb/train/neg/\"):\n",
    "    if count > files:\n",
    "        break\n",
    "    if file.endswith(\".txt\"):\n",
    "        os.rename('files/ch10/aclImdb/train/neg/' + file, 'files/ch10/train/neg2/' + file)\n",
    "    count=count+1\n",
    "count=0\n",
    "for file in os.listdir(\"files/ch10/aclImdb/test/pos/\"):\n",
    "    if count > files:\n",
    "        break\n",
    "    if file.endswith(\".txt\"):\n",
    "        os.rename('files/ch10/aclImdb/test/pos/' + file, 'files/ch10/test/pos2/' + file)\n",
    "    count=count+1\n",
    "count=0\n",
    "for file in os.listdir(\"files/ch10/aclImdb/test/neg/\"):\n",
    "    if count > files:\n",
    "        break\n",
    "    if file.endswith(\".txt\"):\n",
    "        os.rename('files/ch10/aclImdb/test/neg/' + file, 'files/ch10/test/neg2/' + file)\n",
    "    count=count+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW():\n",
    "    '''Implementation of bag of words preprocessing for corpii.\n",
    "    Applies tokenization, removing punctuation and stemming and lemmatizing.\n",
    "    \n",
    "    Returns a list containing the preprocessed documents of all the corpus.\n",
    "    '''\n",
    "    # tokenizing text\n",
    "    text_tokenized = [word_tokenize(doc) for doc in text]\n",
    "    \n",
    "    # removing punctuation\n",
    "    regex = re.compile(f'[{re.escape(string.punctuation)}]')\n",
    "    tokenized_docs_no_punctuation = []\n",
    "    for review in text_tokenized:\n",
    "        new_review = []\n",
    "        for token in review:\n",
    "            new_token = regex.sub(u'', token)\n",
    "            if not new_token == u'':\n",
    "                new_review.append(new_token)\n",
    "        tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "    # stemming and lemmatizing\n",
    "    porter = PorterStemmer()\n",
    "    preprocessed_docs = []\n",
    "    for doc in tokenized_docs_no_punctuation:\n",
    "        final_doc = ''\n",
    "        for word in doc:\n",
    "            final_doc = final_doc + ' ' + porter.stem(word)\n",
    "        preprocessed_docs.append(final_doc)\n",
    "    return preprocessed_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the training data positive\n",
      "Reading the training data negative\n",
      "Defining dictionaries\n",
      "Reading the test data positive\n",
      "Reading the training data negative\n",
      "Computing test feature vectors\n",
      "Training and testing on training Naive Bayes\n",
      "Number of mislabeled training points out of a total 12 points : 0\n",
      "Training and testing on test Naive Bayes\n",
      "Number of mislabeled test points out of a total 12 points : 5\n",
      "Training and testing on train with SVM\n",
      "Number of mislabeled test points out of a total 12 points : 0\n",
      "Testing on test with already trained SVM\n",
      "Number of mislabeled test points out of a total 12 points : 2\n"
     ]
    }
   ],
   "source": [
    "print('Reading the training data positive')\n",
    "text = []\n",
    "for file in os.listdir('files/ch10/train/pos2/'):\n",
    "    if file.endswith('.txt'):\n",
    "        infile = open('files/ch10/train/pos2/' + file, 'br')\n",
    "        text.append(unidecode(infile.read().decode('utf-8')))\n",
    "        infile.close()\n",
    "num_pos_train = len(text)\n",
    "\n",
    "print('Reading the training data negative')\n",
    "for file in os.listdir('files/ch10/train/neg2'):\n",
    "    if file.endswith('.txt'):\n",
    "        infile = open('files/ch10/train/neg2/' + file, 'br')\n",
    "        text.append(unidecode(infile.read().decode('utf-8')))\n",
    "        infile.close()\n",
    "num_train = len(text)\n",
    "\n",
    "print('Defining dictionaries')\n",
    "preprocessed_docs = BoW()\n",
    "\n",
    "# computing TIDF word space\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
    "train_data = tfidf_vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "\n",
    "# reading the test data\n",
    "print('Reading the test data positive')\n",
    "text = []\n",
    "for file in os.listdir('files/ch10/test/pos2/'):\n",
    "    if file.endswith('.txt'):\n",
    "        infile = open('files/ch10/test/pos2/' + file, 'br')\n",
    "        text.append(unidecode(infile.read().decode('utf-8')))\n",
    "        infile.close()\n",
    "num_pos_test = len(text)\n",
    "\n",
    "print('Reading the training data negative')\n",
    "for file in os.listdir('files/ch10/test/neg2'):\n",
    "    if file.endswith('.txt'):\n",
    "        infile = open('files/ch10/test/neg2/' + file, 'br')\n",
    "        text.append(unidecode(infile.read().decode('utf-8')))\n",
    "        infile.close()\n",
    "num_test = len(text)\n",
    "\n",
    "\n",
    "print('Computing test feature vectors')\n",
    "start_time = time.time()\n",
    "\n",
    "preprocessed_docs = BoW()\n",
    "test_data = tfidf_vectorizer.transform(preprocessed_docs)\n",
    "\n",
    "target_train = []\n",
    "for i in range(0, num_pos_train):\n",
    "    target_train.append(0)\n",
    "\n",
    "for i in range(0, num_train - num_pos_train):\n",
    "    target_train.append(1)\n",
    "\n",
    "target_test = []\n",
    "for i in range(0, num_pos_test):\n",
    "    target_test.append(0)\n",
    "for i in range(0, num_test - num_pos_test):\n",
    "    target_test.append(1)\n",
    "\n",
    "\n",
    "print('Training and testing on training Naive Bayes')\n",
    "start_time = time.time()\n",
    "\n",
    "gnb = GaussianNB()\n",
    "test_data.todense()\n",
    "y_pred = gnb.fit(train_data.todense(), target_train).predict(train_data.todense())\n",
    "print(\"Number of mislabeled training points out of a total %d points : %d\" % (train_data.shape[0],(target_train != y_pred).sum()))\n",
    "\n",
    "print('Training and testing on test Naive Bayes')\n",
    "\n",
    "y_pred = gnb.fit(train_data.todense(), target_train).predict(test_data.todense())\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (test_data.shape[0],(target_test != y_pred).sum()))\n",
    "\n",
    "print('Training and testing on train with SVM')\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_data.todense(), target_train)\n",
    "y_pred = clf.predict(train_data.todense())\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (train_data.shape[0],(target_train != y_pred).sum()))\n",
    "\n",
    "print('Testing on test with already trained SVM')\n",
    "y_pred = clf.predict(test_data.todense())\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (test_data.shape[0],(target_test != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('lab': virtualenv)",
   "language": "python",
   "name": "python37464bitlabvirtualenved77d976613b4753a84284c005b6ce98"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
