{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 __Unsupervised Learning__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the problem of unservised learning is that of\n",
    "trying to find hidden structure in unlabeled data. Since the examples\n",
    "fuven to the learner are unlabeled, there is no error or reward signal\n",
    "to evaluate the goodness of a potential solution. This distinguishes unsupervised\n",
    "from supervised learning.\n",
    "__Unsupervised learning is defines as the task performed by algorithms\n",
    "that learn from a training set of unlabeled or unannotated examples, \n",
    "using the features of the inputs to categorize them according to some\n",
    "geometric or statistical criteria.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning encompasses many techniques that seek to summarize and\n",
    "explain key features or structures of the data. Many methods employed in unsuper-\n",
    "vised learning are based on data mining methods used to preprocess data. Most\n",
    "unsupervised learning techniques can be summarized as those that tackle the follow-\n",
    "ing four groups of problems:\n",
    "1. __Clustering__: has a goal to partition the set of examples into groups.\n",
    "2. __Dimensionality reduction__: aims to reduce the dimensionality of the data.\n",
    "    Here we encounter techniques such as PCA, independendent component analysis,\n",
    "    and nonnegative matrix factorization.\n",
    "3. __Outlier detection__: has a purpose to find unusual events (e.g. malfunction)\n",
    "    that distinguish part of the dat afrom the rest according to certain criteria.\n",
    "4. __Novelty detection__: deals with cases when changes occur in the data\n",
    "    (e.g. in steaming data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Clustering__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a process of grouping similar objects together, i.e., to aprtition\n",
    "unlabeled examples into disjoint subsets of clusters, such that:\n",
    "+ Examples withing a clsuter are similar (_high interclass similarity_)\n",
    "+ Exampples in different clusters are different (_low interclass similarity_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we denote data as similar and dissimilat, we should define a measure for this\n",
    "similarity/dissimilarity. Note that grouping similar data together can help\n",
    "in discovering enw categories in an unsupervised manner, even when no sample category\n",
    "lavels are provided. Moreover, two kidns of inputs can be used for grouping:\n",
    "+ in __similarity-based clustering__, the input to the algorithm is an _n_ $\\times$ _n_ \n",
    "    __dissimilarity matrix__ or __distance matrix__.\n",
    "+ in __feature-based clustering__, the input to the algorithm is an _n_ $\\times$ _D_\n",
    "    __feature matrix__ or __design matrix__, where _n_ is the number of examples in the\n",
    "    dataset and _D_ the dimensionality of each example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity-based clustering allows easy inclusion of domain-specific similarity,\n",
    "while feature-based clsutering has the advantage that it is applicable to potentially noisy\n",
    "data.\n",
    "Therefore, several questions regarding the clustering process arise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is a natural grouping among the objects? We need to define the \"groupness\"\n",
    "    and the \"similarity/distance\" between data.\n",
    "* How can we group samples? What are the best procedures? Are they efficient?\n",
    "    Are they fast? Are they deterministic?\n",
    "* How many clusters should we look for in the data? Shall we state his number\n",
    "    _a priori_? Should the process be completely data driven or can the user\n",
    "    guide the grouping process? How can we avoid \"trivial\" clusters? Should we allow\n",
    "    final clustering results to ahve very large or very small clusters?\n",
    "    Which methods work when the number of samples is large? Which methods work when\n",
    "    the number of classes is large?\n",
    "* What constitutes a good grouping? What objective measures can be defined to\n",
    "    evaluate the quality of the clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not always a single or optimal answer to these questions. It used to be said\n",
    "that clustering is a “subjective” issue. Clustering will help us to describe, analyze,\n",
    "and gain insight into the data, but the quality of the partition depends to a great extent\n",
    "on the application and the analyst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Similarity and Distances__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speak of similar and dissimilar data, we need to introduce a notion of the similarity\n",
    "of data. There are several ways for modeling of similarity. A simple way to model\n",
    "this is by means of a Gaussian kernel:\n",
    "\n",
    "$$ s(a, b) = e^{-\\gamma d(a, b)} $$\n",
    "\n",
    "where $d(a, b)$ is a metric function, and $\\gamma$ is a constant that controls the decay\n",
    "of the function. Observe that when $a =b$, the similarity is maximum and equal to one.\n",
    "On the contrary, when $a$ is very different to $b$, the similarity tends to zero.\n",
    "\n",
    "The former modeling of the similarity function suggests that we can use the notion of\n",
    "distance as a surrogate. The most widespread distance metric is the _Minkowski distance__:\n",
    "\n",
    "$$ d(a, b) = \\big( \\sum_{i = 1}^{d} | a_i - b_i |^p \\big)^{1 / p}$$\n",
    "\n",
    "where $d(a, b)$ stands for the distance between two elements $a, b \\in \\mathbb{R}^d $, $d$ is the\n",
    "dimensionality of the data, and $p$ is a parameter.\n",
    "\n",
    "The best-known instantiations of this metric are as follows:\n",
    "\n",
    "* when $p = 2$, we have the _Euclidean distance_\n",
    "* when $p = 1$, we have the _Manhattan distance_\n",
    "* when $p = \\infty$ we have the _max-distance_. In this case the distance\n",
    "    corresponds to the component $|a_b - b_i |$ with the highest value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Metrics to Measure Clustering Quality__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing clustering, the question normally arises: How do we measure the\n",
    "quality of the clustering result? Note that in unsupervised clustering, we do not have\n",
    "groundtruth labels that would allow us to compute the accuracy of the algorithm. Still,\n",
    "there are several procedures for assessing quality. We find two families of techniques:\n",
    "those that allow us to __compare clustering techniques__, and those that __check on specific\n",
    "properties of the clustering__, for example “compactness”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Rand Index, Homogeneity, Completeness and V-measure Scores__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best-known methods for comparing the results in clustering techniques\n",
    "in statistics is the Rand index or Rand measure (named after William M. Rand). The\n",
    "Rand index evaluates the similarity between two results of data clustering. Since\n",
    "in unsupervised clustering, class labels are not known, we use the Rand index to\n",
    "compare the coincidence of different clusterings obtained by different approaches\n",
    "or criteria. As an alternative, we later discuss the Silhouette coefficient: instead of\n",
    "comparing different clusterings, this evaluates the compactness of the results of\n",
    "applying a specific clustering approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of $n$ elements $S = \\{o_1, ..., o_n\\}$, we can compare two partitions of $S$:\n",
    "$X = \\{X_1, ..., X_r\\}$, a partition of $S$ into $r$ subsets; and $Y = \\{Y_1, ..., Y_s\\}$, \n",
    "a partition of $S$ into $s$ subsets. Let us use the annotations as follows:\n",
    "\n",
    "* $a$ is the number of pairs of elements in $S$ that are in the same subset in both\n",
    "    $X$ and $Y$;\n",
    "* $b$ is the number of pairs of elements on $S$ that are in different subsets in both\n",
    "    $X$ and $Y$\n",
    "* $c$ is the number of pairs of elements in $S$ that are in the same subset in $X$, but in\n",
    "    different subsets in $Y$; and\n",
    "* $d$ is the number of pairs of elements in $S$ that are in different subsets in $X$, but\n",
    "    in the same subset in $Y$.\n",
    "    \n",
    "The Rand index, $R$ is defines as:\n",
    "\n",
    "$$ R = \\frac{a + b}{a + b + c + d} $$\n",
    "\n",
    "ensuring that its value is between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems of the Rand index is that when given two datasets with random\n",
    "labelings, it does not take a constant value (e.g., zero) as expected. Moreover, when\n",
    "the number of clusters increases it is desirable that the upper limit tends to the unity.\n",
    "To solve this problem, a form of the Rand index, called the Adjusted Rand index, is\n",
    "used that adjusts the Rand index with respect to chance grouping of elements. It is\n",
    "defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ AR = \\frac{\\binom{n}{2} (a + d) - [(a + b)(a + c) + (c + d)(b + d)]}{\\binom{n}{2}^2[a(a + b)(a + c) + (c + d)(b + d)]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way for comparing clustering results is the __V-measure__. Let us first\n",
    "introduce some concepts. \n",
    "\n",
    "We say that a clustering result satisfies a _homogeneity_\n",
    "criterion if all of its clusters contain only data points which are members of the\n",
    "same original (single) class.\n",
    "\n",
    "A clusterign result satisfies a _completeness__ criterion if all the data points\n",
    "that are members of a given class are elements of the same predicted cluster.\n",
    "\n",
    "Note that both scores have real positive values between 0.0 and 1.0, larger values\n",
    "being desireble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000\n"
     ]
    }
   ],
   "source": [
    "# for example if we consider two toy clustering sets\n",
    "# (e.g. orgiinal and predicted) with four samples and two\n",
    "# labels we get\n",
    "from sklearn import metrics\n",
    "print(f'{metrics.homogeneity_score([0, 0, 1, 1],[0, 0, 0, 0]):.3f}')\n",
    "# the homogeinity is 0 since the sample sin the predicted cluster 0 come from\n",
    "# original cluster 0 and cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'{metrics.completeness_score([0, 0, 1, 1], [1, 1, 0, 0])}')\n",
    "# the completeness is 1 dince all the samples from the original\n",
    "# cluster with label 0 go into the same predicted cluster\n",
    "# with label 1 and all the samples from the original cluster with\n",
    "# label 1 go into the same prdeicted clsiter with label 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, how can we define a measure that takes into account the completeness\n",
    "as well as the homogeneity? The V-measure is the harmonic mean between the\n",
    "homogeneity and the completeness defined as follows:homo\n",
    "\n",
    "$$ v = 2 * (homogeinity * completeness) / (homogeinity + completeness) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this metric is not dependent of the absolute values of the labels: a\n",
    "permutation of the class or cluster label values will not change the score value in\n",
    "any way. Moreover, the metric is symmetric with respect to switching between the\n",
    "predicted and the original cluster label. This is very useful to measure the agreement\n",
    "of two independent label assignment strategies applied to the same dataset even\n",
    "when the real groundtruth is not known. If class members are completely split across\n",
    "different clusters, the assignment is totally incomplete, hence the V-measure is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'{metrics.v_measure_score([0, 0, 0, 0], [0, 1, 2, 3])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, clusters that include samples from different classes destroy the homo-\n",
    "geneity of the labeling, hence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.203426503814917e-16\n"
     ]
    }
   ],
   "source": [
    "print(f'{metrics.v_measure_score([0, 0, 1, 1], [0, 0, 0, 0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we can say that the advantages of the V-measure include that it\n",
    "has bounded scores: 0.0 means the clustering is extremely bad; 1.0 indicates a perfect clustering result. Moreover, it can be interpreted easily: when analyzing the\n",
    "V-measure, low completeness or homogeneity explain in which direction the clustering is not performing well. Furthermore, we do not assume anything about the\n",
    "cluster structure. Therefore, it can be used to compare clustering algorithms such\n",
    "as K-means, which assume isotropic blob shapes, with results of other clustering\n",
    "algorithms such as spectral clustering which can find clusters\n",
    "with “folded” shapes. As a drawback, the previously introduced metrics are not\n",
    "normalized with regard to random labeling. This means that depending on the number of samples, clusters and groundtruth classes, a completely random labeling will\n",
    "not always yield the same values for homogeneity, completeness and hence, the V-measure. In particular, random labeling will not yield a zero score, and they will tend\n",
    "further from zero as the number of clusters increases. It can be shown that this problem can reliably be overcome when the number of samples is high, i.e., more than a\n",
    "thousand, and the number of clusters is less than 10. These metrics require knowledge of the groundtruth classes, while in practice this information is almost never\n",
    "available or requires manual assignment by human annotators. Instead, as mentioned\n",
    "before, these metrics can be used to compare the results of different clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('lab': virtualenv)",
   "language": "python",
   "name": "python37464bitlabvirtualenved77d976613b4753a84284c005b6ce98"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
