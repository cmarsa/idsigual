{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __05 Supervised Learning__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning involves coding programs that automatically adjust their perfor-\n",
    "mance in accordance with their exposure to information in data. This learning is\n",
    "achieved via a parameterized model with tunable parameters that are automatically\n",
    "adjusted according to different performance criteria. Machine learning can be con-\n",
    "sidered a subfield of artificial intelligence (AI) and we can roughly divide the field\n",
    "into the following three major classes.\n",
    "\n",
    "- __Supervised Learning__: Algorithms which learn from a training set of labeled\n",
    "  examples (exemplars) to generalize to the set of all possible inputs. Examples\n",
    "  of techniques in supervised learning:\n",
    "    - logistic regression\n",
    "    - support vector machines\n",
    "    - decision trees\n",
    "    - random forest\n",
    "    - etc\n",
    "- __Unsupervised Learning__: Algorithms that learn from a training set of unlabeled\n",
    "  examples. Used to explore data according to some statistical, geometric or\n",
    "  similarity criterion. Examples of unsupervised learning include:\n",
    "    - k-means clustering\n",
    "    - kernel density estimation\n",
    "    - etc\n",
    "- __Reincforcement learning__: Algorithms that learn via reinforcement from\n",
    "  criticism that provides information on the quality of a solution, but not\n",
    "  on how to improve it. Improved solutions are achieved by iteratively\n",
    "  exploring the solution space.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __The Problem__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we use data from the Lending Club to develop our understanding of\n",
    "machine learning concepts. The Lending Club is a peer-to-peer lending company.\n",
    "It offers loans which are funded by other people. In this sense, the Lending Club\n",
    "acts as a hub connecting borrowers with investors. The client applies for a loan of a\n",
    "certain amount, and the company assesses the risk of the operation. If the application\n",
    "is accepted, it may or may not be fully covered. We will focus on the prediction\n",
    "of whether the loan will be fully funded, based on the scoring of and information\n",
    "related to the application.\n",
    "We will use the partial dataset of period 2007–2011. Framing the problem a little\n",
    "bit more, based on the information supplied by the customer asking for a loan, we\n",
    "want to predict whether it will be granted up to a certain threshold thr . The attributes\n",
    "we use in this problem are related to some of the details of the loan application, such\n",
    "as amount of the loan applied for the borrower, monthly payment to be made by\n",
    "the borrower if the loan is accepted, the borrower’s annual income, the number of\n",
    "incidences of delinquency in the borrower’s credit file, and interest rate of the loan,\n",
    "among others.\n",
    "In this case we would like to predict unsuccessful accepted loans. A loan applica-\n",
    "tion is unsuccessful if the funded amount (funded_amnt) or the amount funded\n",
    "by investors (funded_amnt_inv) falls far short of the requested loan amount\n",
    "(loan_amnt). That is,\n",
    "\n",
    "$$ \\frac{loan - funded}{loan} \\ge 0.95$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this problem we are predicting a binary value: either the loan is fully\n",
    "funded or not. Classification is the natural choice of machine learning tools for\n",
    "prediction with discrete known outcomes. According to the cardinality of the target\n",
    "set, one usually distinguishes between binary classifiers when the target output only\n",
    "takes two values, i.e., the classifier answers questions with a yes or a no; or multiclass\n",
    "classifiers, for a larger number of classes. This issue is important in that not all\n",
    "methods can naturally handle the multiclass setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a formal way, classification is regarded as the problem of finding a function \n",
    "$ h(\\mathbf{x}) = \\mathbb{R}^d \\rightarrow \\mathbb{K}$ that maps an input space in $\\mathbb{R}^d$\n",
    "onto a discrete set of k target outputs or classes $\\mathbb{K} = \\{1, 2, ..., k\\}$.\n",
    "In this setting, the features are arranged as a vector $\\mathbf{x}$ of $d$ real-valued numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encode both target states in a numerical variable, e.g., a successful\n",
    "loan target can take value +1; -1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "ofname = open('input/dataset_small.pkl', 'br')\n",
    "\n",
    "\n",
    "(x, y) = pickle.load(ofname, encoding = 'bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims: 15, samples: 4140\n"
     ]
    }
   ],
   "source": [
    "dims = x.shape[1]\n",
    "N = x.shape[0]\n",
    "print(f'dims: {dims}, samples: {N}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering data arranged as in the previous matrices we refer to:\n",
    "- The columns as features, attributes, dimensions, regressors, covariates, predictors\n",
    "  or independent variables.\n",
    "- the rows as instances, examples or samples\n",
    "- the target as the label, outcome, response or dependent variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted value: -1.0, real target: -1.0\n"
     ]
    }
   ],
   "source": [
    "# apply sk-learn libraries\n",
    "from sklearn import neighbors\n",
    "from sklearn import datasets\n",
    "\n",
    "# create an isntance of k-nearest neighbor classifier\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 11)\n",
    "\n",
    "# train the classifier\n",
    "knn.fit(x, y)\n",
    "\n",
    "# compute the prediction according to the model\n",
    "y_hat = knn.predict(x)\n",
    "\n",
    "# check results\n",
    "print(f'predicted value: {y_hat[-1]}, real target: {y[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic measure of performance of a classifier is its accuracy. This is defined as\n",
    "the number of correctly predicted examples divided by the total amount of examples.\n",
    "\n",
    "$$ acc = \\frac{\\mbox{Number of correct predictions}}{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each estimator has a score() method that invokes the default scoring metric.\n",
    "In the case of k-nearest neighbors, this is the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8316425120772947"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a really good result. But how good is it? Let us first understand a little\n",
    "bit more about the problem by checking the distribution of the labels.\n",
    "Let us load the dataset and check the distribution of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.pie(\n",
    "    np.c_[np.sum(np.where(y == 1, 1, 0)), np.sum(np.where(y == -1, 1, 0))][0],\n",
    "    labels = ['Not fully funded', 'Full amount'],\n",
    "    colors = ['r', 'g'],\n",
    "    shadow = False,\n",
    "    autopct = '%.2f'\n",
    ")\n",
    "plt.gcf().set_size_inches((7, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ith the result observed in Fig. 5.1.\n",
    "Note that there are far more positive labels than negative ones. In this case, the\n",
    "dataset is referred to as unbalanced. This has important consequences for a classifier\n",
    "as we will see later on. In particular, a very simple rule such as always predict the\n",
    "majority class, will give us good performance. In our problem, always predicting\n",
    "that the loan will be fully funded correctly predicts 81.57% of the samples. Observe\n",
    "that this value is very close to that obtained using the classifier.\n",
    "Although accuracy is the most normal metric for evaluating classifiers, there are\n",
    "cases when the business value of correctly predicting elements from one class is\n",
    "different from the value for the prediction of elements of another class. In those\n",
    "cases, accuracy is not a good performance metric and more detailed analysis is\n",
    "needed. The confusion matrix enables us to define different metrics considering such\n",
    "scenarios. The confusion matrix considers the concepts of the classifier outcome and\n",
    "the actual ground truth or gold standard. In a binary problem, there are four possible\n",
    "cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __True Positives (TP)__: When the classifier predicts a sample as positive\n",
    "  and it really is positive.\n",
    "* __False Positives (FP)__: When the classifier predicts a sample as positive\n",
    "  but in fact it is negative.\n",
    "* __True Negative (TN)__: When the classifier predicts a sample as negative\n",
    "  and it really is negative\n",
    "* __False Negative (FN)__: When the classifier predicts a sample as negative\n",
    "  but in fact it is positive.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/conmatr.png\">,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A combination of these elements allows us to define several perfromance metrics:\n",
    "- _Accuracyy_:\n",
    "$$ \\mbox{accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "- Column-wise we find these two partial performance emtrics:\n",
    "    - _Sensitivity_ or _Recall_:\n",
    "    $$ \\mbox{sensitivity} = \\frac{TP}{\\mbox{Real Positives}} = \\frac{TP}{TP + FN} $$\n",
    "    - _Specificity_:\n",
    "    $$ \\mbox{specificity} = \\frac{TN}{\\mbox{Real Negatives}} = \\frac{TP}{TN + FP} $$\n",
    "    \n",
    "- Row-wise we find these two partial performance emtrics:\n",
    "    - _Precision or Positive Predictive Value_:\n",
    "    $$ \\mbox{precision} = \\frac{TP}{\\mbox{Predicted Positives}} = \\frac{TP}{TP + FP} $$\n",
    "    - _Negative Predictive Value_:\n",
    "    $$ \\mbox{NPC} = \\frac{TN}{\\mbox{Predicted Negatives}} = \\frac{TP}{TN + FN} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These partial performance metrics allow us to answer questions concerning how\n",
    "often a classifier predicts a particular class, e.g., what is the rate of predictions for\n",
    "not fully funded loans that have actually not been fully funded? This question is\n",
    "answered by recall. In contrast, we could ask: Of all the fully funded loans predicted\n",
    "by the classifier, how many have been fully funded? This is answered by the precision\n",
    "metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3370,  690],\n",
       "       [   7,   73]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the following example. Let us select a nearest neighbor classifier\n",
    "with the number of neighbors equal to one instead of eleven, as we did before, and\n",
    "check the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n",
      "confusion matrix:\n",
      "[[3377    0]\n",
      " [   0  763]]\n"
     ]
    }
   ],
   "source": [
    "# train a classifier\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 1)\n",
    "knn.fit(x, y)\n",
    "y_hat = knn.predict(x)\n",
    "\n",
    "print(f'accuracy: {metrics.accuracy_score(y_hat, y)}')\n",
    "print(f'confusion matrix:\\n{metrics.confusion_matrix(y_hat, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance measure is perfect! 100% accuracy and a diagonal confusion\n",
    "matrix! This looks good. However, up to this point we have checked the classifier\n",
    "performance on the same data it has been trained with. During exploitation, in real\n",
    "applications, we will use the classifier on data not previously seen. Let us simulate\n",
    "this effect by splitting the data into two sets: one will be used for learning (training\n",
    "set) and the other for testing the accuracy (test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate a real case: Randomize and split data into\n",
    "# two subsets PRC * 100 \\% for training and the rest\n",
    "# (1 - PRC) * 100 \\% for testing\n",
    "perm = np.random.permutation(y.size)\n",
    "PRC = 0.7\n",
    "split_point = int(np.ceil(y.shape[0] * PRC))\n",
    "\n",
    "X_train = x[perm[:split_point].ravel(), :]\n",
    "y_train = y[perm[:split_point].ravel()]\n",
    "\n",
    "X_test = x[perm[split_point:].ravel(), :]\n",
    "y_test = y[perm[split_point:].ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training stats:\n",
      "classification accuracy: 1.0\n",
      "confusion matrix:\n",
      "[[2365    0]\n",
      " [   0  533]]\n"
     ]
    }
   ],
   "source": [
    "# let's train the model with this new partition\n",
    "# train a classifier on training data\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_hat = knn.predict(X_train)\n",
    "\n",
    "print('training stats:')\n",
    "print(f'classification accuracy: {metrics.accuracy_score(y_train, y_hat)}')\n",
    "print(f'confusion matrix:\\n{metrics.confusion_matrix(y_train, y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected from the former experiment, we achieve a perfect score. Now let us\n",
    "see what happens in the simulation with previously unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training stats:\n",
      "classification accuracy: 0.7624798711755234\n",
      "confusion matrix:\n",
      "[[874 138]\n",
      " [157  73]]\n"
     ]
    }
   ],
   "source": [
    "# check on the test set\n",
    "y_hat = knn.predict(X_test)\n",
    "\n",
    "print('training stats:')\n",
    "print(f'classification accuracy: {metrics.accuracy_score(y_test, y_hat)}')\n",
    "print(f'confusion matrix:\\n{metrics.confusion_matrix(y_test, y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good simulation for approximating the test error is to run this process many times and average the performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean expected error: 0.749\n"
     ]
    }
   ],
   "source": [
    "# splitting done by using the tools provided by sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PRC = 0.3\n",
    "acc = np.zeros((10,))\n",
    "for i in range(0, 10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = PRC)\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors = 1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_hat = knn.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(y_hat, y_test)\n",
    "    \n",
    "acc.shape = (1, 10)\n",
    "print(f'mean expected error: {np.mean(acc[0]):.3F}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the resulting error is below 81%, which was the result of the most\n",
    "naive decision process. What is wrong with this result?\n",
    "Let us introduce the nomenclature for the quantities we have just computed and\n",
    "define the following terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __In-sample error__ $E_{in}$: The in.sample error or training error is the error\n",
    "  measured over all the observed data samples in the training set, i.e.,\n",
    "  \n",
    "  $$ E_{in} = \\frac{1}{N}\\sum_{i = 1}^{N} e(x_i, y_i) $$\n",
    "  \n",
    "- __Out-of-sample error__ $E_{out}: he out-of-sample error or generalization error mea-\n",
    "    sures the expected error on unseen data. We can approximate/simulate this quantity\n",
    "    by holding back some training data for testing purposes.\n",
    "   \n",
    "  $$ E_{out} = \\mathbb{E}_{x, y} (e(x, y)) $$\n",
    "  \n",
    "Note that the definition of the instantaneous error e(x i , y i ) is still missing. For\n",
    "example, in classification we could use the indicator function to account for a cor-\n",
    "rectly classified sample as follows:\n",
    "\n",
    "$$ e(x_i, y_i) = I[h(x_i) = y_i] = \\begin{cases}\n",
    "        \\begin{array}{lcl}\n",
    "        1, \\mbox{if  } h(x_i) = y_i \\\\\n",
    "        0 \\mbox{  otherwise}\n",
    "        \\end{array}\n",
    "\\end{cases}  $$\n",
    "\n",
    "Observe that:\n",
    "\n",
    "$$  E_{out} \\ge E_{in} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select \"best model\" by choosing the one with\n",
    "# lowest error rate\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "\n",
    "PRC = 0.1\n",
    "acc_r = np.zeros((10, 4))\n",
    "\n",
    "for i in range(0, 10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = PRC)\n",
    "    nn1 = neighbors.KNeighborsClassifier(n_neighbors = 1)\n",
    "    nn3 = neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "    svc = svm.SVC()\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    nn1.fit(X_train, y_train)\n",
    "    nn3.fit(X_train, y_train)\n",
    "    svc.fit(X_train, y_train)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    yhat_nn1 = nn1.predict(X_test)\n",
    "    yhat_nn3 = nn3.predict(X_test)\n",
    "    yhat_svc = svc.predict(X_test)\n",
    "    yhat_dt = dt.predict(X_test)\n",
    "    \n",
    "    acc_r[i][0] = metrics.accuracy_score(yhat_nn1, y_test)\n",
    "    acc_r[i][1] = metrics.accuracy_score(yhat_nn3, y_test)\n",
    "    acc_r[i][2] = metrics.accuracy_score(yhat_svc, y_test)\n",
    "    acc_r[i][3] = metrics.accuracy_score(yhat_dt, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7705314 , 0.78019324, 0.8115942 , 0.76811594],\n",
       "       [0.78019324, 0.8236715 , 0.81884058, 0.80434783],\n",
       "       [0.7705314 , 0.79710145, 0.81642512, 0.76570048],\n",
       "       [0.74637681, 0.78019324, 0.80434783, 0.77777778],\n",
       "       [0.80434783, 0.81642512, 0.8236715 , 0.76328502],\n",
       "       [0.77294686, 0.79227053, 0.82608696, 0.80193237],\n",
       "       [0.76086957, 0.7705314 , 0.79468599, 0.75603865],\n",
       "       [0.76086957, 0.80193237, 0.83091787, 0.76086957],\n",
       "       [0.76811594, 0.78743961, 0.80434783, 0.74879227],\n",
       "       [0.76811594, 0.8115942 , 0.83816425, 0.7705314 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(\n",
    "    acc_r, whis=True,\n",
    "    showcaps=True,\n",
    "    labels=('1-NN', '3-NN', 'SVM', 'Decision Tree'),\n",
    "    patch_artist=True,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is one particular form of a general model selection technique called\n",
    "cross-validation. There are other kinds of cross-validation, such as leave-one-out or\n",
    "K-fold cross-validation.\n",
    " - In leave-one-out, given N samples, the model is trained with N − 1 samples and\n",
    "tested with the remaining one. This is repeated N times, once per training sample\n",
    "and the result is averaged.\n",
    " - In K-fold cross-validation, the training set is divided into K nonoverlapping splits.\n",
    "K-1 splits are used for training and the remaining one used to assess the mean.\n",
    "This process is repeated K times leaving one split out each time. The results are\n",
    "then averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Learning?\n",
    "Let us recall the two basic values defined in the last section. We talk of training error\n",
    "or in-sample error, $E_{in}$ , which refers to the error measured over all the observed data\n",
    "samples in the training set. We also talk of test error or generalization error, $E_{out}$ ,\n",
    "as the error expected on unseen data.\n",
    "We can empirically estimate the generalization error by means of cross-validation\n",
    "techniques and observe that:\n",
    "\n",
    "$$ E_{out} \\ge E_{in} $$\n",
    "\n",
    "The goal of learning is to minimize the generalization error; but how can we\n",
    "guarantee this minimization using only training data?\n",
    "From the above inequality it is easy to derive a couple of very intuitive ideas.\n",
    "\n",
    " - Because $E_{out}$ is greater than or equal to $E_{in}$ it is desirible to have\n",
    " $$ E_{in} \\rightarrow 0 $$\n",
    " \n",
    " - Addidionally, we also want the training error behaviour to track the\n",
    "   generalization error so tha tif one minimizes the n-sample error the out-of-sample\n",
    "   error follows, i.e.,\n",
    " $$ E_{out} \\approx E_{in} $$\n",
    " \n",
    " We can write the second condition as\n",
    " $$ E_{in} \\le E_{out} \\le E_{in} + \\Omega $$\n",
    " \n",
    " with $\\Omega \\rightarrow 0 $.\n",
    " \n",
    "We would like to characterize $\\Omega$ in terms of oyr problem parameters, i.e., \n",
    "the number of samples (N), dimensionality of the problem (d), etc.\n",
    "Statistical analysis offers an interesting characterization of this quantity\n",
    "\n",
    "$$ E_{out} \\le E_{in}(C) + \\mathcal{O} \\Big( \\sqrt{\\frac{log C}{N}} \\Big)$$\n",
    "\n",
    "Where $C$ is a measure of the complexity of the model class we are using. Technically,\n",
    "we may also refer to this model class as the hypothesis space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation in Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 10-fold cross-valdiation set\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(\n",
    "    n_splits = 10,\n",
    "    shuffle = True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# search for the parameter among the following\n",
    "C = np.arange(2, 20)\n",
    "\n",
    "acc = np.zeros((10, 18))\n",
    "i = 0\n",
    "for train_index, val_index in kf.split(x):\n",
    "    X_train, X_val = x[train_index], x[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    j = 0\n",
    "    for c in C:\n",
    "        dt = tree.DecisionTreeClassifier(\n",
    "            min_samples_leaf=1,\n",
    "            max_depth=c\n",
    "        )\n",
    "        dt.fit(X_train, y_train)\n",
    "        yhat = dt.predict(X_val)\n",
    "        acc[i][j] = metrics.accuracy_score(yhat, y_val)\n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  [0.81460707 0.81370163 0.80947203 0.80312853 0.80071707 0.79618899\n",
      " 0.79105576 0.78471408 0.78290412 0.7780712  0.76750264 0.76871656\n",
      " 0.76418666 0.76057038 0.75392294 0.75604048 0.75241419 0.7460807 ]\n",
      "selected model index: 0\n"
     ]
    }
   ],
   "source": [
    "# train_Test split\n",
    "X = x.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "# create a 10-fold cross-validation set\n",
    "fold = KFold(\n",
    "    n_splits=10,\n",
    "    shuffle=True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# search the parameter among the following\n",
    "C = np.arange(2, 20)\n",
    "acc = np.zeros((10, 18))\n",
    "i = 0\n",
    "for train_index, val_index in fold.split(X_train):\n",
    "    X_t, X_val = X_train[train_index - 1], X_train[val_index - 1]\n",
    "    y_t, y_val = y_train[train_index], y_train[val_index]\n",
    "    j = 0\n",
    "    for c in C:\n",
    "        dt = tree.DecisionTreeClassifier(\n",
    "            min_samples_leaf=1,\n",
    "            max_depth = c\n",
    "        )\n",
    "        dt.fit(X_t, y_t)\n",
    "        yhat = dt.predict(X_val)\n",
    "        acc[i][j] = metrics.accuracy_score(yhat, y_val)\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "\n",
    "print('mean accuracy: ', np.mean(acc, axis = 0))\n",
    "\n",
    "print('selected model index:', np.argmax(np.mean(acc, axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.8236714975845411\n"
     ]
    }
   ],
   "source": [
    "# train the model with the complete trainin set \n",
    "# with the selected complexity\n",
    "dt = tree.DecisionTreeClassifier(\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=C[np.argmax(np.mean(acc, axis = 0))]\n",
    ")\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# test the model with the test set\n",
    "yhat = dt.predict(X_test)\n",
    "print('test accuracy: ', metrics.accuracy_score(yhat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train final model\n",
    "dt = tree.DecisionTreeClassifier(\n",
    "    min_samples_leaf = 1,\n",
    "    max_depth = C[np.argmax(np.mean(acc, axis = 0))]\n",
    ")\n",
    "dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalities Concerning Learning Models\n",
    "Before going into some of the details of the models selected, let us check the com-\n",
    "ponents of any learning algorithm. In order to be able to learn, an algorithm has to\n",
    "define at least three components:\n",
    "\n",
    "- _The model class/hypothesis space_ defines the family of mathematical models that will\n",
    "  be used. The target decision boundary will be approximated from one element of this\n",
    "  space. For example, we can consider the class of lienar models. In this case our decision\n",
    "  boundary will be a line if the problem is defined in __R²__ and the model class is the space of all\n",
    "  possible lines in __R²__.\n",
    "  Model classes define the geometric properties of the decision function. There are different\n",
    "  taxonomies but the best known are the families of _linear_ and _nonlinear_ models.\n",
    "  These families usually depend on some parameters; and the solution to a learning problem\n",
    "  is the selection of a particular set of parameters, i.e., the selection of an instance\n",
    "  of a model from the model class space. The model class space is also\n",
    "  called the _hypothesis space_.\n",
    "  The selection of the best model will depend on our problem and what we want to obtain from the problem.\n",
    "  The primary goal in learning is usually to achieve the minimum error/maximum performance; but\n",
    "  according to what else we want from the algorithm, we can come up with different algorithms.\n",
    "  Other common desireble properties are interpretability, behaviour when faced with missing data,\n",
    "  fast training, etc.\n",
    "  \n",
    "- _The problem model_ formalizes and encodes the desired properties of the solution.\n",
    "  In many cases, this formalization takes the form of an optimization problem. In its most\n",
    "  basic instantiation, the problem model can be the _minimization of an error function_. The error\n",
    "  function measures the difference between oyr mdoel and the target. Informally speaking, in a\n",
    "  classifiaction problem it measures how \"irritated\" we are when our model misses the right\n",
    "  label for a training sample.\n",
    "  The problem model can also be sued to impose other constraints on our solution, such as finding\n",
    "  a smooth approximation, a model with a low degree of small complexity, a sparse colution, etc.\n",
    "  \n",
    "- _The learning algorithm_ is an optimization/search method or algorithm that, given a model class,\n",
    "  fits it to the training data ccording to the error function. According to the nature of our\n",
    "  problem there are many different algorithms. In general, we are talking about finding the minimum\n",
    "  error approximation or maximum probable model. In those cases, if the problem is convex/quasi-convex\n",
    "  we will typically use first-or second order methods (i.e., gradient descent, coordinate descent,\n",
    "  Newton's method, interior point methods, etc). Other searching techniques such as genetic algorithms\n",
    "  or Monte Carlo techniques can be used if we do not have access to the derivatives of the\n",
    "  objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __GridSearch validation on SVMs and Random Forests__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856280193236715\n",
      "[[3372  590]\n",
      " [   5  173]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'C': [1e4, 1e5, 1e6],\n",
    "    'gamma': [1e-5, 1e-4, 1e-3]\n",
    "}\n",
    "\n",
    "kf = KFold(\n",
    "    n_splits = 5,\n",
    "    shuffle=True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "acc = np.zeros((5, ))\n",
    "i = 0\n",
    "\n",
    "# we will build the predicted y from the partial\n",
    "# predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index, :], X[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    clf = svm.SVC(kernel = 'rbf')\n",
    "    clf = GridSearchCV(clf, parameters, cv = 3)\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    X_test = scaler.transform(X_test)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    \n",
    "print(metrics.accuracy_score(yhat, y))\n",
    "print(metrics.confusion_matrix(yhat, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result obtained has a large error in the non-fully funded class (negative). This\n",
    "is because the default scoring for cross-validation grid-search is mean accuracy.\n",
    "Depending on our business, this large error in recall for this class may be unaccept-\n",
    "able. There are different strategies for diminishing the impact of this effect. On the\n",
    "one hand, we may change the default scoring and find the parameter setting that cor-\n",
    "responds to the maximum average recall. On the other hand, we could mitigate this\n",
    "effect by imposing a different weight on an error on the critical class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('lab': virtualenv)",
   "language": "python",
   "name": "python37464bitlabvirtualenved77d976613b4753a84284c005b6ce98"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
